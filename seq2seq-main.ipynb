{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.导入实验所需的第三方模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "import random\n",
    "import unicodedata\n",
    "import math\n",
    "\n",
    "from mindspore import Tensor, nn, Model, context\n",
    "from mindspore.train.serialization import load_param_into_net, load_checkpoint\n",
    "from mindspore.train.callback import LossMonitor, CheckpointConfig, ModelCheckpoint, TimeMonitor\n",
    "from mindspore import dataset as ds\n",
    "from mindspore.mindrecord import FileWriter\n",
    "from mindspore import Parameter\n",
    "from mindspore.nn.loss.loss import _Loss\n",
    "from mindspore.ops import functional as F\n",
    "from mindspore.ops import operations as P\n",
    "from mindspore.common import dtype as mstype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.设置运行环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.set_context(mode=context.GRAPH_MODE, save_graphs=False, device_target='Ascend')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.查看数据内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi.\t嗨。\n",
      "\n",
      "Hi.\t你好。\n",
      "\n",
      "Run.\t你用跑的。\n",
      "\n",
      "Wait!\t等等！\n",
      "\n",
      "Hello!\t你好。\n",
      "\n",
      "I try.\t让我来。\n",
      "\n",
      "I won!\t我赢了。\n",
      "\n",
      "Oh no!\t不会吧。\n",
      "\n",
      "Cheers!\t干杯!\n",
      "\n",
      "He ran.\t他跑了。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#查看训练数据内容前10行内容\n",
    "with open(\"cmn_zhsim.txt\", 'r', encoding='utf-8') as f:\n",
    "        for i in range(10):\n",
    "            print(f.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.定义数据预处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS = \"<eos>\"\n",
    "SOS = \"<sos>\"\n",
    "MAX_SEQ_LEN=10\n",
    "#我们需要将字符转化为ASCII编码\n",
    "#并全部转化为小写字母，并修剪大部分标点符号\n",
    "#除了(a-z, A-Z, \".\", \"?\", \"!\", \",\")这些字符外，全替换成空格\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = s.lower().strip()\n",
    "    s = unicodeToAscii(s)\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "def prepare_data(data_path, vocab_save_path, max_seq_len):\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    # 读取文本文件，按行分割，再将每行分割成语句对\n",
    "    data = data.split('\\n')\n",
    "\n",
    "     # 截取前2000行数据进行训练\n",
    "    data = data[:2000]\n",
    "\n",
    "    # 分割每行中的中英文\n",
    "    en_data = [normalizeString(line.split('\\t')[0]) for line in data]\n",
    "\n",
    "    ch_data = [line.split('\\t')[1] for line in data]\n",
    "\n",
    "    # 利用集合，获得中英文词汇表\n",
    "    en_vocab = set(' '.join(en_data).split(' '))\n",
    "    id2en = [EOS] + [SOS] + list(en_vocab)\n",
    "    en2id = {c:i for i,c in enumerate(id2en)}\n",
    "    en_vocab_size = len(id2en)\n",
    "    # np.savetxt(os.path.join(vocab_save_path, 'en_vocab.txt'), np.array(id2en), fmt='%s')\n",
    "\n",
    "    ch_vocab = set(''.join(ch_data))\n",
    "    id2ch = [EOS] + [SOS] + list(ch_vocab)\n",
    "    ch2id = {c:i for i,c in enumerate(id2ch)}\n",
    "    ch_vocab_size = len(id2ch)\n",
    "    # np.savetxt(os.path.join(vocab_save_path, 'ch_vocab.txt'), np.array(id2ch), fmt='%s')\n",
    "\n",
    "    # 将句子用词汇表id表示\n",
    "    en_num_data = np.array([[1] + [int(en2id[en]) for en in line.split(' ')] + [0] for line in en_data])\n",
    "    ch_num_data = np.array([[1] + [int(ch2id[ch]) for ch in line] + [0] for line in ch_data])\n",
    "\n",
    "    #将短句子扩充到统一的长度\n",
    "    for i in range(len(en_num_data)):\n",
    "        num = max_seq_len + 1 - len(en_num_data[i])\n",
    "        if(num >= 0):\n",
    "            en_num_data[i] += [0]*num\n",
    "        else:\n",
    "            en_num_data[i] = en_num_data[i][:max_seq_len] + [0]\n",
    "\n",
    "    for i in range(len(ch_num_data)):\n",
    "        num = max_seq_len + 1 - len(ch_num_data[i])\n",
    "        if(num >= 0):\n",
    "            ch_num_data[i] += [0]*num\n",
    "        else:\n",
    "            ch_num_data[i] = ch_num_data[i][:max_seq_len] + [0]\n",
    "    \n",
    "    \n",
    "    np.savetxt(os.path.join(vocab_save_path, 'en_vocab.txt'), np.array(id2en), fmt='%s')\n",
    "    \n",
    "    np.savetxt(os.path.join(vocab_save_path, 'ch_vocab.txt'), np.array(id2ch), fmt='%s')\n",
    "\n",
    "    return en_num_data, ch_num_data, en_vocab_size, ch_vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.获得mindrecord文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将处理后的数据保存为mindrecord文件，方便后续训练\n",
    "def convert_to_mindrecord(data_path, mindrecord_save_path, max_seq_len):\n",
    "    en_num_data, ch_num_data, en_vocab_size, ch_vocab_size = prepare_data(data_path, mindrecord_save_path, max_seq_len)\n",
    "\n",
    "    # 输出前十行英文句子对应的数据\n",
    "    for i in range(10):\n",
    "        print(en_num_data[i])\n",
    "    \n",
    "    data_list_train = []\n",
    "    for en, ch in zip(en_num_data, ch_num_data):\n",
    "        en = np.array(en).astype(np.int32)\n",
    "        ch = np.array(ch).astype(np.int32)\n",
    "        data_json = {\"encoder_data\": en.reshape(-1),\n",
    "                     \"decoder_data\": ch.reshape(-1)}\n",
    "        data_list_train.append(data_json)\n",
    "    \n",
    "    data_list_eval = random.sample(data_list_train, 20)\n",
    "\n",
    "    data_dir = os.path.join(mindrecord_save_path, \"gru_train.mindrecord\")\n",
    "    writer = FileWriter(data_dir)\n",
    "    schema_json = {\"encoder_data\": {\"type\": \"int32\", \"shape\": [-1]},\n",
    "                   \"decoder_data\": {\"type\": \"int32\", \"shape\": [-1]}}\n",
    "    writer.add_schema(schema_json, \"gru_schema\")\n",
    "    writer.write_raw_data(data_list_train)\n",
    "    writer.commit()\n",
    "\n",
    "    data_dir = os.path.join(mindrecord_save_path, \"gru_eval.mindrecord\")\n",
    "    writer = FileWriter(data_dir)\n",
    "    writer.add_schema(schema_json, \"gru_schema\")\n",
    "    writer.write_raw_data(data_list_eval)\n",
    "    writer.commit()\n",
    "\n",
    "    print(\"en_vocab_size: \", en_vocab_size)\n",
    "    print(\"ch_vocab_size: \", ch_vocab_size)\n",
    "\n",
    "    return en_vocab_size, ch_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 570, 84, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 570, 84, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 423, 84, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 178, 461, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 309, 461, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 320, 717, 84, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 320, 547, 461, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1043, 506, 461, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 728, 461, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1050, 41, 84, 0, 0, 0, 0, 0, 0, 0]\n",
      "en_vocab_size:  1154\n",
      "ch_vocab_size:  1116\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1154, 1116)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not os.path.exists(\"./preprocess\"):\n",
    "    os.mkdir('./preprocess')\n",
    "convert_to_mindrecord(\"cmn_zhsim.txt\", './preprocess', MAX_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.超参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easydict import EasyDict as edict\n",
    "\n",
    "# CONFIG\n",
    "cfg = edict({\n",
    "    'en_vocab_size': 1154,\n",
    "    'ch_vocab_size': 1116,\n",
    "    'max_seq_length': 10,\n",
    "    'hidden_size': 1024,\n",
    "    'batch_size': 16,\n",
    "    'eval_batch_size': 1,\n",
    "    'learning_rate': 0.001,\n",
    "    'momentum': 0.9,\n",
    "    'num_epochs': 15,\n",
    "    'save_checkpoint_steps': 125,\n",
    "    'keep_checkpoint_max': 10,\n",
    "    'dataset_path':'./preprocess',\n",
    "    'ckpt_save_path':'./ckpt',\n",
    "    'checkpoint_path':'./ckpt/gru-15_125.ckpt'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.读取mindrecord，定义数据集生成函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_operation(encoder_data, decoder_data):\n",
    "    encoder_data = encoder_data[1:]\n",
    "    target_data = decoder_data[1:]\n",
    "    decoder_data = decoder_data[:-1]\n",
    "    return encoder_data, decoder_data, target_data\n",
    "\n",
    "def eval_operation(encoder_data, decoder_data):\n",
    "    encoder_data = encoder_data[1:]\n",
    "    decoder_data = decoder_data[:-1]\n",
    "    return encoder_data, decoder_data\n",
    "\n",
    "def create_dataset(data_home, batch_size, repeat_num=1, is_training=True, device_num=1, rank=0):\n",
    "    if is_training:\n",
    "        data_dir = os.path.join(data_home, \"gru_train.mindrecord\")\n",
    "    else:\n",
    "        data_dir = os.path.join(data_home, \"gru_eval.mindrecord\")\n",
    "    data_set = ds.MindDataset(data_dir, columns_list=[\"encoder_data\",\"decoder_data\"], \n",
    "                              num_parallel_workers=4,\n",
    "                              num_shards=device_num, shard_id=rank)\n",
    "    if is_training:\n",
    "        operations = target_operation\n",
    "        data_set = data_set.map(operations=operations, \n",
    "                                input_columns=[\"encoder_data\",\"decoder_data\"],\n",
    "                    output_columns=[\"encoder_data\",\"decoder_data\",\"target_data\"],\n",
    "                    column_order=[\"encoder_data\",\"decoder_data\",\"target_data\"])\n",
    "    else:\n",
    "        operations = eval_operation\n",
    "        data_set = data_set.map(operations=operations, \n",
    "                                input_columns=[\"encoder_data\",\"decoder_data\"],\n",
    "                   output_columns=[\"encoder_data\",\"decoder_data\"],\n",
    "                   column_order=[\"encoder_data\",\"decoder_data\"])\n",
    "    data_set = data_set.shuffle(buffer_size=data_set.get_dataset_size())\n",
    "    data_set = data_set.batch(batch_size=batch_size, drop_remainder=True)\n",
    "    data_set = data_set.repeat(count=repeat_num)\n",
    "    return data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = create_dataset(cfg.dataset_path, cfg.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.定义GRU单元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru_default_state(batch_size, input_size, hidden_size, num_layers=1, bidirectional=False):\n",
    "    '''Weight init for gru cell'''\n",
    "    stdv = 1 / math.sqrt(hidden_size)\n",
    "    weight_i = Parameter(Tensor(\n",
    "        np.random.uniform(-stdv, stdv, (input_size, 3*hidden_size)).astype(np.float32)), \n",
    "                         name='weight_i')\n",
    "    weight_h = Parameter(Tensor(\n",
    "        np.random.uniform(-stdv, stdv, (hidden_size, 3*hidden_size)).astype(np.float32)), \n",
    "                         name='weight_h')\n",
    "    bias_i = Parameter(Tensor(\n",
    "        np.random.uniform(-stdv, stdv, (3*hidden_size)).astype(np.float32)), name='bias_i')\n",
    "    bias_h = Parameter(Tensor(\n",
    "        np.random.uniform(-stdv, stdv, (3*hidden_size)).astype(np.float32)), name='bias_h')\n",
    "    return weight_i, weight_h, bias_i, bias_h\n",
    "\n",
    "class GRU(nn.Cell):\n",
    "    def __init__(self, config, is_training=True):\n",
    "        super(GRU, self).__init__()\n",
    "        if is_training:\n",
    "            self.batch_size = config.batch_size\n",
    "        else:\n",
    "            self.batch_size = config.eval_batch_size\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.weight_i, self.weight_h, self.bias_i, self.bias_h = \\\n",
    "            gru_default_state(self.batch_size, self.hidden_size, self.hidden_size)\n",
    "        self.rnn = P.DynamicGRUV2()\n",
    "        self.cast = P.Cast()\n",
    "\n",
    "    def construct(self, x, hidden):\n",
    "        x = self.cast(x, mstype.float16)\n",
    "        y1, h1, _, _, _, _ = self.rnn(x, self.weight_i, self.weight_h, self.bias_i, self.bias_h, None, hidden)\n",
    "        return y1, h1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.定义Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Cell):\n",
    "    def __init__(self, config, is_training=True):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.vocab_size = config.en_vocab_size\n",
    "        self.hidden_size = config.hidden_size\n",
    "        if is_training:\n",
    "            self.batch_size = config.batch_size\n",
    "        else:\n",
    "            self.batch_size = config.eval_batch_size\n",
    "\n",
    "        self.trans = P.Transpose()\n",
    "        self.perm = (1, 0, 2)\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.hidden_size)\n",
    "        self.gru = GRU(config, is_training=is_training).to_float(mstype.float16)\n",
    "        self.h = Tensor(np.zeros((self.batch_size, self.hidden_size)).astype(np.float16))\n",
    "\n",
    "    def construct(self, encoder_input):\n",
    "        embeddings = self.embedding(encoder_input)\n",
    "        embeddings = self.trans(embeddings, self.perm)\n",
    "        output, hidden = self.gru(embeddings, self.h)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.定义Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Cell):\n",
    "    def __init__(self, config, is_training=True, dropout=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.vocab_size = config.ch_vocab_size\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.max_len = config.max_seq_length\n",
    "\n",
    "        self.trans = P.Transpose()\n",
    "        self.perm = (1, 0, 2)\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(1-dropout)\n",
    "        self.attn = nn.Dense(self.hidden_size, self.max_len)\n",
    "        self.softmax = nn.Softmax(axis=2)\n",
    "        self.bmm = P.BatchMatMul()\n",
    "        self.concat = P.Concat(axis=2)\n",
    "        self.attn_combine = nn.Dense(self.hidden_size * 2, self.hidden_size)\n",
    "\n",
    "        self.gru = GRU(config, is_training=is_training).to_float(mstype.float16)\n",
    "        self.out = nn.Dense(self.hidden_size, self.vocab_size)\n",
    "        self.logsoftmax = nn.LogSoftmax(axis=2)\n",
    "        self.cast = P.Cast()\n",
    "\n",
    "    def construct(self, decoder_input, hidden, encoder_output):\n",
    "        embeddings = self.embedding(decoder_input)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        # calculate attn\n",
    "        attn_weights = self.softmax(self.attn(embeddings)) # [1,1,10]\n",
    "        encoder_output = self.trans(encoder_output, self.perm)\n",
    "        attn_applied = self.bmm(attn_weights, self.cast(encoder_output,mstype.float32))\n",
    "        output =  self.concat((embeddings, attn_applied))\n",
    "        output = self.attn_combine(output)\n",
    "\n",
    "\n",
    "        embeddings = self.trans(embeddings, self.perm)\n",
    "        output, hidden = self.gru(embeddings, hidden)\n",
    "        output = self.cast(output, mstype.float32)\n",
    "        output = self.out(output)\n",
    "        output = self.logsoftmax(output)\n",
    "\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.定义Seq2Seq整体结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Cell):\n",
    "    def __init__(self, config, is_train=True):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.max_len = config.max_seq_length\n",
    "        self.is_train = is_train\n",
    "\n",
    "        self.encoder = Encoder(config, is_train)\n",
    "        self.decoder = Decoder(config, is_train)\n",
    "        self.expanddims = P.ExpandDims()\n",
    "        self.squeeze = P.Squeeze(axis=0)\n",
    "        self.argmax = P.ArgMaxWithValue(axis=int(2), keep_dims=True)\n",
    "        self.concat = P.Concat(axis=1)\n",
    "        self.concat2 = P.Concat(axis=0)\n",
    "        self.select = P.Select()\n",
    "\n",
    "    def construct(self, src, dst):\n",
    "        encoder_output, hidden = self.encoder(src)\n",
    "        decoder_hidden = self.squeeze(encoder_output[self.max_len-2:self.max_len-1:1, ::, ::])\n",
    "        if self.is_train:\n",
    "            outputs, _ = self.decoder(dst, decoder_hidden, encoder_output)\n",
    "        else:\n",
    "            decoder_input = dst[::,0:1:1]\n",
    "            decoder_outputs = ()\n",
    "            for i in range(0, self.max_len):\n",
    "                decoder_output, decoder_hidden, _ = self.decoder(decoder_input, \n",
    "                                                                 decoder_hidden, encoder_output)\n",
    "                decoder_hidden = self.squeeze(decoder_hidden)\n",
    "                decoder_output, _ = self.argmax(decoder_output)\n",
    "                decoder_output = self.squeeze(decoder_output)\n",
    "                decoder_outputs += (decoder_output,)\n",
    "                decoder_input = decoder_output\n",
    "            outputs = self.concat(decoder_outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLLLoss(_Loss):\n",
    "    '''\n",
    "       NLLLoss function\n",
    "    '''\n",
    "    def __init__(self, reduction='mean'):\n",
    "        super(NLLLoss, self).__init__(reduction)\n",
    "        self.one_hot = P.OneHot()\n",
    "        self.reduce_sum = P.ReduceSum()\n",
    "\n",
    "    def construct(self, logits, label):\n",
    "        label_one_hot = self.one_hot(label, F.shape(logits)[-1], F.scalar_to_array(1.0), \n",
    "                                     F.scalar_to_array(0.0))\n",
    "        #print('NLLLoss label_one_hot:',label_one_hot, label_one_hot.shape)\n",
    "        #print('NLLLoss logits:',logits, logits.shape)\n",
    "        #print('xxx:', logits * label_one_hot)\n",
    "        loss = self.reduce_sum(-1.0 * logits * label_one_hot, (1,))\n",
    "        return self.get_loss(loss)\n",
    "    \n",
    "class WithLossCell(nn.Cell):\n",
    "    def __init__(self, backbone, config):\n",
    "        super(WithLossCell, self).__init__(auto_prefix=False)\n",
    "        self._backbone = backbone\n",
    "        self.batch_size = config.batch_size\n",
    "        self.onehot = nn.OneHot(depth=config.ch_vocab_size)\n",
    "        self._loss_fn = NLLLoss()\n",
    "        self.max_len = config.max_seq_length\n",
    "        self.squeeze = P.Squeeze()\n",
    "        self.cast = P.Cast()\n",
    "        self.argmax = P.ArgMaxWithValue(axis=1, keep_dims=True)\n",
    "        self.print = P.Print()\n",
    "\n",
    "    def construct(self, src, dst, label):\n",
    "        out = self._backbone(src, dst)\n",
    "        loss_total = 0\n",
    "        for i in range(self.batch_size):\n",
    "            loss = self._loss_fn(self.squeeze(out[::,i:i+1:1,::]), \n",
    "                                 self.squeeze(label[i:i+1:1, ::]))\n",
    "            loss_total += loss\n",
    "        loss = loss_total / self.batch_size\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.定义训练网络与优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(6455:281473841743744,MainProcess):2022-12-02-17:37:37.155.50 [mindspore/nn/loss/loss.py:166] '_Loss' is deprecated from version 1.3 and will be removed in a future version, use 'LossBase' instead.\n"
     ]
    }
   ],
   "source": [
    "network = Seq2Seq(cfg)\n",
    "network = WithLossCell(network, cfg)\n",
    "optimizer = nn.Adam(network.trainable_params(), learning_rate=cfg.learning_rate, beta1=0.9, beta2=0.98)\n",
    "model = Model(network, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.定义回调函数、构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_cb = LossMonitor()\n",
    "config_ck = CheckpointConfig(save_checkpoint_steps=cfg.save_checkpoint_steps, keep_checkpoint_max=cfg.keep_checkpoint_max)\n",
    "ckpoint_cb = ModelCheckpoint(prefix=\"gru\", directory=cfg.ckpt_save_path, config=config_ck)\n",
    "time_cb = TimeMonitor(data_size=ds_train.get_dataset_size())\n",
    "callbacks = [time_cb, ckpoint_cb, loss_cb]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.启动训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] DEVICE(6455,ffffbc59c780,python):2022-12-02-17:37:49.986.242 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[DropoutGenMask] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(6455,ffffbc59c780,python):2022-12-02-17:37:50.113.041 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[StridedSliceGrad] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(6455,ffffbc59c780,python):2022-12-02-17:37:50.113.440 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[StridedSliceGrad] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(6455,ffffbc59c780,python):2022-12-02-17:37:50.113.822 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[StridedSliceGrad] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(6455,ffffbc59c780,python):2022-12-02-17:37:50.114.199 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[StridedSliceGrad] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(6455,ffffbc59c780,python):2022-12-02-17:37:50.114.578 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[StridedSliceGrad] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(6455,ffffbc59c780,python):2022-12-02-17:37:50.114.955 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[StridedSliceGrad] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(6455,ffffbc59c780,python):2022-12-02-17:37:50.115.333 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[StridedSliceGrad] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(6455,ffffbc59c780,python):2022-12-02-17:37:50.115.707 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[StridedSliceGrad] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(6455,ffffbc59c780,python):2022-12-02-17:37:50.116.083 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[StridedSliceGrad] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(6455,ffffbc59c780,python):2022-12-02-17:37:50.116.461 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[StridedSliceGrad] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(6455,ffffbc59c780,python):2022-12-02-17:37:50.116.836 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[StridedSliceGrad] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(6455,ffffbc59c780,python):2022-12-02-17:37:50.117.225 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[StridedSliceGrad] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(6455,ffffbc59c780,python):2022-12-02-17:37:50.117.604 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[StridedSliceGrad] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(6455,ffffbc59c780,python):2022-12-02-17:37:50.117.978 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[StridedSliceGrad] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(6455,ffffbc59c780,python):2022-12-02-17:37:50.118.347 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[StridedSliceGrad] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(6455,ffffbc59c780,python):2022-12-02-17:37:50.118.720 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[StridedSliceGrad] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(6455,ffffbc59c780,python):2022-12-02-17:37:50.485.850 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[StridedSliceGrad] don't support int64, reduce precision from int64 to int32.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 125, loss is 3.026381254196167\n",
      "epoch time: 22011.396 ms, per step time: 176.091 ms\n",
      "epoch: 2 step: 125, loss is 2.206502914428711\n",
      "epoch time: 5567.654 ms, per step time: 44.541 ms\n",
      "epoch: 3 step: 125, loss is 1.7297335863113403\n",
      "epoch time: 5678.133 ms, per step time: 45.425 ms\n",
      "epoch: 4 step: 125, loss is 1.787466049194336\n",
      "epoch time: 5471.636 ms, per step time: 43.773 ms\n",
      "epoch: 5 step: 125, loss is 1.4309864044189453\n",
      "epoch time: 5458.441 ms, per step time: 43.668 ms\n",
      "epoch: 6 step: 125, loss is 1.1279113292694092\n",
      "epoch time: 5498.023 ms, per step time: 43.984 ms\n",
      "epoch: 7 step: 125, loss is 0.7117752432823181\n",
      "epoch time: 5495.964 ms, per step time: 43.968 ms\n",
      "epoch: 8 step: 125, loss is 0.6191931366920471\n",
      "epoch time: 5775.537 ms, per step time: 46.204 ms\n",
      "epoch: 9 step: 125, loss is 0.5680367350578308\n",
      "epoch time: 5440.476 ms, per step time: 43.524 ms\n",
      "epoch: 10 step: 125, loss is 0.33588194847106934\n",
      "epoch time: 5642.062 ms, per step time: 45.136 ms\n",
      "epoch: 11 step: 125, loss is 0.200953871011734\n",
      "epoch time: 5501.930 ms, per step time: 44.015 ms\n",
      "epoch: 12 step: 125, loss is 0.18558579683303833\n",
      "epoch time: 5822.965 ms, per step time: 46.584 ms\n",
      "epoch: 13 step: 125, loss is 0.1497543603181839\n",
      "epoch time: 5548.404 ms, per step time: 44.387 ms\n",
      "epoch: 14 step: 125, loss is 0.14832618832588196\n",
      "epoch time: 5470.390 ms, per step time: 43.763 ms\n",
      "epoch: 15 step: 125, loss is 0.12445802986621857\n",
      "epoch time: 5536.198 ms, per step time: 44.290 ms\n"
     ]
    }
   ],
   "source": [
    "model.train(cfg.num_epochs, ds_train, callbacks=callbacks, dataset_sink_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.定义推理网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferCell(nn.Cell):\n",
    "    def __init__(self, network, config):\n",
    "        super(InferCell, self).__init__(auto_prefix=False)\n",
    "        self.expanddims = P.ExpandDims()\n",
    "        self.network = network\n",
    "\n",
    "    def construct(self, src, dst):\n",
    "        out = self.network(src, dst)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Seq2Seq(cfg,is_train=False)\n",
    "network = InferCell(network, cfg)\n",
    "network.set_train(False)\n",
    "parameter_dict = load_checkpoint(cfg.checkpoint_path)\n",
    "load_param_into_net(network, parameter_dict)\n",
    "model = Model(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17.定义翻译测试函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#打开词汇表\n",
    "with open(os.path.join(cfg.dataset_path,\"en_vocab.txt\"), 'r', encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "en_vocab = list(data.split('\\n'))\n",
    "\n",
    "with open(os.path.join(cfg.dataset_path,\"ch_vocab.txt\"), 'r', encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "ch_vocab = list(data.split('\\n'))\n",
    "\n",
    "def translate(str_en):\n",
    "    max_seq_len = 10\n",
    "    str_vocab = normalizeString(str_en).split(' ')\n",
    "    print(\"English\",str(str_vocab))\n",
    "    str_id = [1]\n",
    "    for i in str_vocab:\n",
    "        str_id += [en_vocab.index(i)]\n",
    "   \n",
    "    num = max_seq_len + 1 - len(str_id)\n",
    "    if(num >= 0):\n",
    "        str_id += [0]*num\n",
    "    else:\n",
    "        str_id = str_id[:max_seq_len] + [0]\n",
    "    str_id = Tensor(np.array([str_id[1:]]).astype(np.int32))\n",
    "   \n",
    "    out_id = [1]+[0]*10\n",
    "    out_id = Tensor(np.array([out_id[:-1]]).astype(np.int32))\n",
    "    \n",
    "    output = network(str_id, out_id)\n",
    "    out= ''\n",
    "    for x in output[0].asnumpy():\n",
    "        if x == 0:\n",
    "            break\n",
    "        out += ch_vocab[x]\n",
    "    print(\"中文\",out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.离线加载预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English ['i', 'love', 'tom']\n",
      "中文 我爱这首歌。\n"
     ]
    }
   ],
   "source": [
    "translate('i love tom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
